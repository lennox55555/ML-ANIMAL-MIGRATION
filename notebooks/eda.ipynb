{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, time\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/GNP_Aerial_counting_1969_2022.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_cols = ['MALE', 'CALVES'] #columns that are empty\n",
    "zero_cols = ['LINE2002', 'LINE2012', 'COLLAR', 'CONSERVANC', 'SANCTUARY'] #columns that are > 80% just 0s\n",
    "drop_cols = ['NOTES'] # other columns to drop\n",
    "df.drop(columns=empty_cols, inplace=True)\n",
    "df.drop(columns=zero_cols, inplace=True)\n",
    "df.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TIME'] = df['TIME'].apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second if pd.notna(x) else x)\n",
    "df['TIME'] = df['TIME'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TYPE'] = df['TYPE'].map({'Fixed-wing': 0, 'Helicopter': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero_count = (df['COUNT_DAY'] == 0).sum()\n",
    "#print(zero_count / df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATE'] = df['DATE'].apply(lambda t: t.day if isinstance(t, datetime) else np.nan)\n",
    "df['DATE'] = pd.to_numeric(df['DATE'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_mapping = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4,\n",
    "    'May': 5, 'June': 6, 'July': 7, 'August': 8,\n",
    "    'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "df['MONTH'] = df['MONTH'].map(month_mapping)\n",
    "df['MONTH'] = pd.to_numeric(df['MONTH'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lat_lag1'] = df.groupby('SPECIES')['LATITUDE'].shift(1)\n",
    "df['lon_lag1'] = df.groupby('SPECIES')['LONGITUDE'].shift(1)\n",
    "df['lat_lag2'] = df.groupby('SPECIES')['LATITUDE'].shift(2)\n",
    "df['lon_lag2'] = df.groupby('SPECIES')['LONGITUDE'].shift(2)\n",
    "df['count_lag1'] = df.groupby('SPECIES')['COUNT_DAY'].shift(1)\n",
    "df['count_lag2'] = df.groupby('SPECIES')['COUNT_DAY'].shift(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SPECIES'] = df['SPECIES'].str.lower()\n",
    "df['STRATUM'] = df['STRATUM'].str.lower()\n",
    "df = pd.get_dummies(df, columns=['SPECIES', 'STRATUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncorrelation_matrix = df.corr()\\nplt.figure(figsize=(25, 25)) \\nsns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\\nplt.title(\"Correlation Matrix Heatmap\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(25, 25)) \n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-DL Model\n",
    "\n",
    "#### Run Cross-Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year being tested for CV: 2002\n",
      "Mean Squared Error: 0.011390783293928121\n",
      "R² Score: 0.766421487847372\n",
      "year being tested for CV: 2012\n",
      "Mean Squared Error: 0.448780217578266\n",
      "R² Score: 0.7319068516867674\n",
      "year being tested for CV: 2018\n",
      "Mean Squared Error: 0.012596944950188373\n",
      "R² Score: 0.7797733530405973\n",
      "year being tested for CV: 1972\n",
      "Mean Squared Error: 0.004966876743004004\n",
      "R² Score: 0.7389944974429555\n",
      "year being tested for CV: 2004\n",
      "Mean Squared Error: 0.05149120966480534\n",
      "R² Score: 0.22744298911616698\n",
      "Average Mean Squared Error: 0.10584520644603837\n",
      "Average R² Score: 0.6489078358267719\n"
     ]
    }
   ],
   "source": [
    "def run_cv(df, num_splits=5):\n",
    "    tested_years = [2022]\n",
    "    ave_mse = 0\n",
    "    ave_r2 = 0\n",
    "\n",
    "    for _ in range(num_splits):\n",
    "        cv_year = random.choice(df['COUNT'].unique())\n",
    "        while cv_year in tested_years:\n",
    "            cv_year = random.choice(df['COUNT'].unique())\n",
    "        \n",
    "        tested_years.append(cv_year)\n",
    "\n",
    "        train_df = df[~df['COUNT'].isin([2022, cv_year])]\n",
    "        test_df = df[df['COUNT'] == cv_year]\n",
    "\n",
    "        #fillna with mean\n",
    "        train_df = train_df.fillna(train_df.mean())\n",
    "        test_df = test_df.fillna(test_df.mean())\n",
    "\n",
    "        X_train = train_df.drop(columns=['ID', 'LATITUDE', 'LONGITUDE', 'COUNT_DAY'])\n",
    "        y_train = train_df[['COUNT_DAY', 'LATITUDE', 'LONGITUDE']]\n",
    "        X_test = test_df.drop(columns=['ID', 'LATITUDE', 'LONGITUDE', 'COUNT_DAY'])\n",
    "        y_test = test_df[['COUNT_DAY', 'LATITUDE', 'LONGITUDE']]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Multi-output regressor\n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        print(f\"year being tested for CV: {cv_year}\")\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "        # Example: R² for predictions\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"R² Score: {r2}\")\n",
    "\n",
    "        ave_mse += mse\n",
    "        ave_r2 += r2\n",
    "\n",
    "    print(f\"Average Mean Squared Error: {ave_mse / num_splits}\")\n",
    "    print(f\"Average R² Score: {ave_r2 / num_splits}\")\n",
    "\n",
    "run_cv(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on Full Dataset (Excluding 2022 for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.008737311556722062\n",
      "R² Score: 0.9444638811245224\n"
     ]
    }
   ],
   "source": [
    "# Get Train Test Split\n",
    "train_df = df[df['COUNT'] != 2022]\n",
    "test_df = df[df['COUNT'] == 2022]\n",
    "\n",
    "#fillna with mean\n",
    "train_df = train_df.fillna(train_df.mean())\n",
    "test_df = test_df.fillna(test_df.mean())\n",
    "\n",
    "X_train = train_df.drop(columns=['ID', 'LATITUDE', 'LONGITUDE', 'COUNT_DAY'])\n",
    "y_train = train_df[['COUNT_DAY', 'LATITUDE', 'LONGITUDE']]\n",
    "X_test = test_df.drop(columns=['ID', 'LATITUDE', 'LONGITUDE', 'COUNT_DAY'])\n",
    "y_test = test_df[['COUNT_DAY', 'LATITUDE', 'LONGITUDE']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Multi-output regressor\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Example: Calculate MSE for your model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Example: R² for predictions\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-Up NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Convert features and target to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class MultiOutputNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size=3):\n",
    "        super(MultiOutputNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32) \n",
    "        self.output = nn.Linear(32, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 1 NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiOutputNN(input_size=X_train_tensor.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 58.2837\n",
      "Epoch 2/100, Loss: 0.3902\n",
      "Epoch 3/100, Loss: 0.2790\n",
      "Epoch 4/100, Loss: 0.2228\n",
      "Epoch 5/100, Loss: 0.1812\n",
      "Epoch 6/100, Loss: 0.1587\n",
      "Epoch 7/100, Loss: 0.1428\n",
      "Epoch 8/100, Loss: 0.1284\n",
      "Epoch 9/100, Loss: 0.1153\n",
      "Epoch 10/100, Loss: 0.1052\n",
      "Epoch 11/100, Loss: 0.0942\n",
      "Epoch 12/100, Loss: 0.0864\n",
      "Epoch 13/100, Loss: 0.0806\n",
      "Epoch 14/100, Loss: 0.0718\n",
      "Epoch 15/100, Loss: 0.0690\n",
      "Epoch 16/100, Loss: 0.0634\n",
      "Epoch 17/100, Loss: 0.0595\n",
      "Epoch 18/100, Loss: 0.0595\n",
      "Epoch 19/100, Loss: 0.0526\n",
      "Epoch 20/100, Loss: 0.0509\n",
      "Epoch 21/100, Loss: 0.0484\n",
      "Epoch 22/100, Loss: 0.0460\n",
      "Epoch 23/100, Loss: 0.0453\n",
      "Epoch 24/100, Loss: 0.0442\n",
      "Epoch 25/100, Loss: 0.0428\n",
      "Epoch 26/100, Loss: 0.0395\n",
      "Epoch 27/100, Loss: 0.0391\n",
      "Epoch 28/100, Loss: 0.0373\n",
      "Epoch 29/100, Loss: 0.0377\n",
      "Epoch 30/100, Loss: 0.0349\n",
      "Epoch 31/100, Loss: 0.0354\n",
      "Epoch 32/100, Loss: 0.0337\n",
      "Epoch 33/100, Loss: 0.0329\n",
      "Epoch 34/100, Loss: 0.0318\n",
      "Epoch 35/100, Loss: 0.0317\n",
      "Epoch 36/100, Loss: 0.0311\n",
      "Epoch 37/100, Loss: 0.0307\n",
      "Epoch 38/100, Loss: 0.0303\n",
      "Epoch 39/100, Loss: 0.0294\n",
      "Epoch 40/100, Loss: 0.0290\n",
      "Epoch 41/100, Loss: 0.0292\n",
      "Epoch 42/100, Loss: 0.0289\n",
      "Epoch 43/100, Loss: 0.0279\n",
      "Epoch 44/100, Loss: 0.0288\n",
      "Epoch 45/100, Loss: 0.0288\n",
      "Epoch 46/100, Loss: 0.0271\n",
      "Epoch 47/100, Loss: 0.0264\n",
      "Epoch 48/100, Loss: 0.0265\n",
      "Epoch 49/100, Loss: 0.0260\n",
      "Epoch 50/100, Loss: 0.0258\n",
      "Epoch 51/100, Loss: 0.0254\n",
      "Epoch 52/100, Loss: 0.0252\n",
      "Epoch 53/100, Loss: 0.0249\n",
      "Epoch 54/100, Loss: 0.0240\n",
      "Epoch 55/100, Loss: 0.0248\n",
      "Epoch 56/100, Loss: 0.0237\n",
      "Epoch 57/100, Loss: 0.0256\n",
      "Epoch 58/100, Loss: 0.0240\n",
      "Epoch 59/100, Loss: 0.0245\n",
      "Epoch 60/100, Loss: 0.0249\n",
      "Epoch 61/100, Loss: 0.0235\n",
      "Epoch 62/100, Loss: 0.0229\n",
      "Epoch 63/100, Loss: 0.0228\n",
      "Epoch 64/100, Loss: 0.0230\n",
      "Epoch 65/100, Loss: 0.0222\n",
      "Epoch 66/100, Loss: 0.0229\n",
      "Epoch 67/100, Loss: 0.0223\n",
      "Epoch 68/100, Loss: 0.0222\n",
      "Epoch 69/100, Loss: 0.0219\n",
      "Epoch 70/100, Loss: 0.0213\n",
      "Epoch 71/100, Loss: 0.0218\n",
      "Epoch 72/100, Loss: 0.0208\n",
      "Epoch 73/100, Loss: 0.0214\n",
      "Epoch 74/100, Loss: 0.0211\n",
      "Epoch 75/100, Loss: 0.0215\n",
      "Epoch 76/100, Loss: 0.0212\n",
      "Epoch 77/100, Loss: 0.0201\n",
      "Epoch 78/100, Loss: 0.0207\n",
      "Epoch 79/100, Loss: 0.0202\n",
      "Epoch 80/100, Loss: 0.0200\n",
      "Epoch 81/100, Loss: 0.0204\n",
      "Epoch 82/100, Loss: 0.0200\n",
      "Epoch 83/100, Loss: 0.0200\n",
      "Epoch 84/100, Loss: 0.0197\n",
      "Epoch 85/100, Loss: 0.0198\n",
      "Epoch 86/100, Loss: 0.0196\n",
      "Epoch 87/100, Loss: 0.0197\n",
      "Epoch 88/100, Loss: 0.0201\n",
      "Epoch 89/100, Loss: 0.0200\n",
      "Epoch 90/100, Loss: 0.0194\n",
      "Epoch 91/100, Loss: 0.0194\n",
      "Epoch 92/100, Loss: 0.0192\n",
      "Epoch 93/100, Loss: 0.0189\n",
      "Epoch 94/100, Loss: 0.0196\n",
      "Epoch 95/100, Loss: 0.0197\n",
      "Epoch 96/100, Loss: 0.0185\n",
      "Epoch 97/100, Loss: 0.0193\n",
      "Epoch 98/100, Loss: 0.0192\n",
      "Epoch 99/100, Loss: 0.0183\n",
      "Epoch 100/100, Loss: 0.0182\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.22942012548446655\n",
      "Mean Absolute Error: 0.21979950368404388\n",
      "R² Score: 0.8434914946556091\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    actual = y_test_tensor.numpy()\n",
    "    y_preds = model(X_test_tensor)\n",
    "    #y_preds[:, 0] = torch.round(y_preds[:, 0])\n",
    "    predicted = y_preds.detach().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    print(f\"Mean Absolute Error: {mae}\")    \n",
    "\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble NNs (So far better than 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [MultiOutputNN(input_size=X_train.shape[1]) for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for Model 1: 0.6375\n",
      "Loss for Model 2: 0.6034\n",
      "Loss for Model 3: 0.6245\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "epochs = 100\n",
    "\n",
    "trained_models = []\n",
    "for i, model in enumerate(models):\n",
    "    tot_loss = 0.0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        tot_loss += running_loss / len(train_loader)\n",
    "\n",
    "    print(f\"Loss for Model {i+1}: {tot_loss / 100:.4f}\")\n",
    "    trained_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.009994489140808582\n",
      "Mean Absolute Error: 0.05950450524687767\n",
      "R² Score: 0.9502697587013245\n"
     ]
    }
   ],
   "source": [
    "# Predict with all models\n",
    "predictions = []\n",
    "\n",
    "for model in trained_models:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_preds = model(X_test_tensor)\n",
    "        predictions.append(y_preds.detach().numpy())\n",
    "\n",
    "# Average predictions\n",
    "predicted = sum(predictions) / len(predictions)\n",
    "\n",
    "actual = y_test_tensor.numpy()\n",
    "mse = mean_squared_error(actual, predicted)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "mae = mean_absolute_error(actual, predicted)\n",
    "print(f\"Mean Absolute Error: {mae}\")    \n",
    "\n",
    "r2 = r2_score(actual, predicted)\n",
    "print(f\"R² Score: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
